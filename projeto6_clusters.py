# -*- coding: utf-8 -*-
"""[A]_Projeto6_Clusters_Desmatamento_UsoSolo_Gustavo_Millena_(4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10avcc_j_rjgmCeeuKfl9fPNfNKE4pZRQ

# Projeto Final - CDII (FACOM32701)

**Tema:** PROJETO 6 - Análise de Agrupamentos de Desmatamento e Uso do Solo  
**Grupo:**   
Gustavo Antunes de Souza - 12311BSI276  
Millena Lins dos Santos - 12311BSI292  

---

## Objetivos do projeto

Este projeto aplica técnicas não supervisionadas para identificar padrões naturais de desmatamento e uso/cobertura do solo entre municípios brasileiros, com foco em:

- Como os padrões de desmatamento se diferenciam entre biomas;
- Identificação de padrões de “desmatamento evitado” (conservação/baixa pressão);
- Relação entre desmatamento e indicadores de uso/cobertura do solo.

---

## Fontes de dados (públicas)

- **INPE / TerraBrasilis (PRODES):** dados anuais de desmatamento por município (arquivos `terrabrasilis_*.csv`).  
- **IBGE:** bioma predominante por município (`Bioma_Predominante_por_Municipio_2024.csv`).  
- **MapBiomas (Coleção 10.1):** estatísticas de uso/cobertura por município (arquivo `MAPBIOMAS_BRAZIL-COVERAGE_STATISTICS-COL.10.1-MUNICIPALITIES_STATES_BIOMES.xlsx`).

> **Execução no Colab:** faça upload dos arquivos na pasta do ambiente (painel à esquerda - Files - Upload).
"""

# =========================
# 1) Imports e configurações
# =========================
import os
import glob
import re
import unicodedata
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score,
    adjusted_rand_score,
    normalized_mutual_info_score,
)

from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# =======================================
# 2) Funções utilitárias (padronização etc)
# =======================================
def normalize_text(s: str) -> str:
    """Normaliza texto para facilitar merges por nome (remove acentos, espaços duplos, caixa)."""
    if pd.isna(s):
        return ""
    s = str(s).strip().upper()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = re.sub(r"\s+", " ", s)
    return s

def parse_ptbr_number(x):
    """Converte número no padrão pt-BR (1.234,56) para float (1234.56)."""
    if pd.isna(x):
        return np.nan
    s = str(x).strip()
    s = s.replace(".", "").replace(",", ".")
    try:
        return float(s)
    except ValueError:
        return np.nan

def find_first_existing(candidates):
    for p in candidates:
        if os.path.exists(p):
            return p
    return None

"""## 3) Carregamento dos dados

### 3.1 PRODES / TerraBrasilis (desmatamento por município)

Neste projeto, usamos os arquivos com colunas:
- `mun` (nome do município)
- `geocode_ibge` (código IBGE)
- `year`
- `area km²` (área desmatada no ano)

> Observação: para cada bioma existem múltiplos CSVs; será selecionado automaticamente o arquivo com maior número de municípios para cada bioma.

"""

# ==================================================
# 3.1) PRODES / TerraBrasilis - leitura e consolidação
# ==================================================
tb_files = sorted(glob.glob("terrabrasilis_*.csv"))
print(f"Arquivos TerraBrasilis encontrados: {len(tb_files)}")
tb_files[:10]

def infer_biome_from_filename(fname: str) -> str:
    base = os.path.basename(fname).lower()
    if "legal_amazon" in base:
        return "Amazônia Legal"
    if "amazon_nf" in base:
        return "Amazônia (Não Floresta)"
    if "amazon" in base:
        return "Amazônia"
    if "cerrado" in base:
        return "Cerrado"
    if "caatinga" in base:
        return "Caatinga"
    if "mata_atlantica" in base:
        return "Mata Atlântica"
    if "pantanal" in base:
        return "Pantanal"
    if "pampa" in base:
        return "Pampa"
    return "Desconhecido"

def load_terrabrasilis_csv(path: str) -> pd.DataFrame:
    with open(path, "r", encoding="utf-8", errors="ignore") as f:
        first = f.readline()
    sep = ";" if ";" in first else ","
    df = pd.read_csv(path, sep=sep)
    df.columns = [c.strip() for c in df.columns]

    # área
    area_candidates = [c for c in df.columns if "area" in c.lower()]
    if area_candidates:
        area_col = area_candidates[0]
        df[area_col] = df[area_col].map(parse_ptbr_number)
        df = df.rename(columns={area_col: "area_km2"})

    # geocode
    geo_candidates = [c for c in df.columns if "geocode" in c.lower()]
    if geo_candidates:
        df = df.rename(columns={geo_candidates[0]: "geocode_ibge"})

    return df

# seleciona apenas arquivos anuais com município + geocode + year
tb_annual = []
for p in tb_files:
    try:
        d = load_terrabrasilis_csv(p)
    except Exception:
        continue
    cols = set([c.lower() for c in d.columns])
    if {"mun", "geocode_ibge", "year"}.issubset(cols) and ("area_km2" in d.columns):
        d["bioma_origem"] = infer_biome_from_filename(p)
        d["source_file"] = os.path.basename(p)
        keep_cols = ["geocode_ibge", "mun", "uf", "year", "area_km2", "bioma_origem", "source_file"]
        d = d[[c for c in keep_cols if c in d.columns]]
        tb_annual.append(d)

print("Arquivos TerraBrasilis anuais (município) detectados:", len(tb_annual))

# Para cada bioma, escolhe o arquivo anual mais completo (maior número de municípios distintos)
tb_best = []
for biome in sorted({d["bioma_origem"].iloc[0] for d in tb_annual}):
    dfs = [d for d in tb_annual if d["bioma_origem"].iloc[0] == biome]
    best = max(dfs, key=lambda x: x["geocode_ibge"].nunique())
    tb_best.append(best)

prodes_long = pd.concat(tb_best, ignore_index=True)
prodes_long["geocode_ibge"] = prodes_long["geocode_ibge"].astype(str).str.replace(r"\D", "", regex=True)

print("Biomas/recortes selecionados:", prodes_long["bioma_origem"].unique())
print("Anos disponíveis:", int(prodes_long["year"].min()), "→", int(prodes_long["year"].max()))
display(prodes_long.groupby("bioma_origem")["geocode_ibge"].nunique().sort_values(ascending=False).to_frame("n_municipios"))
prodes_long.head()

"""### 3.2 IBGE - Bioma predominante por município

Usamos o arquivo do IBGE para mapear, por código IBGE, o bioma predominante de cada município.

"""

# ============================
# 3.2) IBGE - Bioma Predominante
# ============================

import pandas as pd
import numpy as np
import unicodedata
import re

ibge_candidates = [
    "Bioma_Predominante_por_Municipio_2024.csv",
    "Bioma_Predominante_por_Municipio_2024.xlsx",
    "Lista_Municipio_Bioma_250mil.xls",
]

ibge_path = find_first_existing(ibge_candidates)
print("Arquivo IBGE selecionado:", ibge_path)

if ibge_path is None:
    raise FileNotFoundError(
        "Arquivo do IBGE não encontrado. Faça upload de "
        "'Bioma_Predominante_por_Municipio_2024.csv' (ou equivalente)."
    )

def normalize_colname(s: str) -> str:
    s = str(s)
    s = unicodedata.normalize("NFKD", s).encode("ascii", "ignore").decode("utf-8")
    s = s.strip().lower()
    # corrige leetspeak/artefatos comuns (caso apareça algo tipo 'igeoca3digo')
    s = s.replace("3", "o").replace("0", "o").replace("1", "l").replace("5", "s")
    s = re.sub(r"\s+", " ", s)
    return s

def read_ibge_robust(path: str) -> pd.DataFrame:
    if path.lower().endswith(".csv"):
        # tenta separadores comuns; IBGE frequentemente é ';'
        for sep in [";", ",", "\t", "|"]:
            try:
                df_try = pd.read_csv(path, dtype=str, sep=sep, encoding="utf-8-sig", engine="python")
                # se veio só 1 coluna gigante, não é esse separador
                if df_try.shape[1] > 1:
                    return df_try
            except Exception:
                pass
        # fallback final: deixa o pandas tentar
        return pd.read_csv(path, dtype=str, encoding="utf-8-sig", engine="python")
    else:
        return pd.read_excel(path, dtype=str)

ibge = read_ibge_robust(ibge_path)

# normaliza nomes das colunas
ibge.columns = [normalize_colname(c) for c in ibge.columns]
print("Colunas detectadas no IBGE:", ibge.columns.tolist())

# detectar coluna de geocódigo (mesmo se estiver “estranha”)
geo_col = None
for c in ibge.columns:
    cl = re.sub(r"[^a-z]", "", c)  # só letras
    # pega qualquer variação com "geocod" (geocodigo, geocod, etc.)
    if "geocod" in cl or cl in ("cdgeocmu", "codigoibge", "codibge"):
        geo_col = c
        break

# fallback: procura colunas que contenham "geo" e "cod"
if geo_col is None:
    for c in ibge.columns:
        if ("geo" in c) and ("cod" in c or "codigo" in c):
            geo_col = c
            break

if geo_col is None:
    raise KeyError(
        "Não foi possível detectar a coluna de geocódigo no IBGE. "
        f"Colunas disponíveis: {ibge.columns.tolist()}"
    )

# detectar coluna de bioma
biome_col = None
for c in ibge.columns:
    if "bioma" in c:
        biome_col = c
        break

if biome_col is None:
    raise KeyError(
        "Não foi possível detectar a coluna de bioma no IBGE. "
        f"Colunas disponíveis: {ibge.columns.tolist()}"
    )

# detectar coluna de UF (útil para merge com MapBiomas por município+UF)
uf_col = None
for c in ibge.columns:
    if c in ("sigla da uf", "sigla uf", "uf", "estado", "sg_uf"):
        uf_col = c
        break
if uf_col is None:
    # tenta achar algo que tenha "uf"
    for c in ibge.columns:
        if "uf" == c or c.endswith(" uf") or "sigla" in c and "uf" in c:
            uf_col = c
            break

# detectar coluna de nome do município (útil para merge)
mun_col = None
for c in ibge.columns:
    if ("municip" in c) or ("munic" in c) or ("nome do" in c and "mun" in c):
        mun_col = c
        break

ibge_clean_cols = [geo_col, biome_col] + ([uf_col] if uf_col else []) + ([mun_col] if mun_col else [])
ibge_clean = ibge[ibge_clean_cols].copy()

ibge_clean = ibge_clean.rename(columns={
    geo_col: "geocode_ibge",
    biome_col: "bioma_ibge",
    **({uf_col: "uf_ibge"} if uf_col else {}),
    **({mun_col: "mun_ibge"} if mun_col else {}),
})

# limpa geocode -> só dígitos
ibge_clean["geocode_ibge"] = (
    ibge_clean["geocode_ibge"]
    .astype(str)
    .str.replace(r"\D", "", regex=True)
)

# remove geocodes inválidos
ibge_clean = ibge_clean.dropna(subset=["geocode_ibge"])
ibge_clean = ibge_clean[ibge_clean["geocode_ibge"].str.len().between(6, 7)]
ibge_clean = ibge_clean.drop_duplicates("geocode_ibge")

display(ibge_clean.head())
print("Municípios únicos no IBGE:", ibge_clean["geocode_ibge"].nunique())

"""### 3.3 MapBiomas - uso/cobertura por município (Coleção 10.1)

O arquivo `COVERAGE_10.1` contém uma linha por município X classe e colunas de anos (1985…2024), com valores de área.

Como o MapBiomas pode não trazer o código IBGE diretamente no arquivo municipal, faremos o merge pelo **nome do município + UF** (normalizados).

"""

# ============================
# 3.3) MapBiomas - coverage stats
# ============================

import pandas as pd
import numpy as np
import re
import unicodedata

mb_candidates = [
    "MAPBIOMAS_BRAZIL-COVERAGE_STATISTICS-COL.10.1-MUNICIPALITIES_STATES_BIOMES.xlsx",
    "MAPBIOMAS_BRAZIL-COVERAGE_STATISTICS-COL.10.1-MUNICIPALITIES_STATES_BIOMES (1).xlsx",
]
mb_path = find_first_existing(mb_candidates)
print("Arquivo MapBiomas selecionado:", mb_path)

if mb_path is None:
    raise FileNotFoundError("Arquivo MapBiomas não encontrado. Faça upload do .xlsx municipal.")

mb = pd.read_excel(mb_path, sheet_name="COVERAGE_10.1")

print("Colunas detectadas no MapBiomas (COVERAGE_10.1):")
print(list(mb.columns))

# detectar anos (colunas '1985'...'2024')
year_cols = [c for c in mb.columns if re.fullmatch(r"\d{4}", str(c).strip())]
year_cols = sorted(year_cols, key=lambda x: int(str(x)))

print("Total de linhas MapBiomas:", mb.shape[0])
print("Anos detectados:", (year_cols[0], year_cols[-1]) if year_cols else "NENHUM")
print("Total anos:", len(year_cols))

required_cols = ["municipality", "state_acronym", "biome", "class_level_0", "class_level_1"]
missing = [c for c in required_cols if c not in mb.columns]
if missing:
    raise KeyError(
        f"Colunas esperadas não encontradas na aba COVERAGE_10.1: {missing}. "
        f"Colunas disponíveis: {mb.columns.tolist()}"
    )

# normalização de texto (para merges)
def normalize_text(s):
    s = "" if pd.isna(s) else str(s)
    s = unicodedata.normalize("NFKD", s).encode("ascii", "ignore").decode("utf-8")
    s = s.strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s

mb["mun_norm"] = mb["municipality"].map(normalize_text)
mb["uf_norm"] = mb["state_acronym"].map(normalize_text)

display(mb.head())

# Agregação por macro-classe (class_level_0) e uma proxy de agropecuária (Farming)
mb_work = mb.copy()
mb_work["mun_norm"] = mb_work["municipality"].map(normalize_text)
mb_work["uf_norm"] = mb_work["state_acronym"].map(normalize_text)

mb_long = mb_work[["mun_norm", "uf_norm", "class_level_0", "class_level_1"] + year_cols].copy()

mb_l0 = mb_long.groupby(["mun_norm", "uf_norm", "class_level_0"], as_index=False)[year_cols].sum()

mb_farming = mb_long[mb_long["class_level_1"].astype(str).str.contains("Farming", case=False, na=False)].copy()
mb_farming = mb_farming.groupby(["mun_norm", "uf_norm"], as_index=False)[year_cols].sum()
mb_farming["class_level_0"] = "Farming (MapBiomas)"
mb_l0 = pd.concat([mb_l0, mb_farming[["mun_norm","uf_norm","class_level_0"]+year_cols]], ignore_index=True)

mb_l0.head()

"""## 4) Construção do dataset final (município agregado)

### 4.1 Features de desmatamento (PRODES)

Agregamos a série anual por município criando indicadores:

- Total no período
- Média, mediana, desvio padrão
- Tendência temporal (slope)
- Pressão recente (últimos 3 anos)
- Anos com desmatamento zero (proxy de conservação)

"""

# ============================
# 4.1) Features PRODES por município
# ============================
prodes = prodes_long.copy()
prodes["mun_norm"] = prodes["mun"].map(normalize_text)
prodes["uf_norm"] = prodes["uf"].map(normalize_text)

ymin, ymax = int(prodes["year"].min()), int(prodes["year"].max())
recent_years = list(range(max(ymin, ymax-2), ymax+1))

def slope_time(y, x):
    if len(y) < 2:
        return np.nan
    x = np.array(x, dtype=float)
    y = np.array(y, dtype=float)
    x = x - x.mean()
    denom = (x**2).sum()
    if denom == 0:
        return np.nan
    return (x*y).sum()/denom

agg = []
for (geo, mun_norm, uf_norm), g in prodes.groupby(["geocode_ibge","mun_norm","uf_norm"]):
    g = g.sort_values("year")
    areas = g["area_km2"].values
    years = g["year"].values

    total = np.nansum(areas)
    mean = np.nanmean(areas)
    median = np.nanmedian(areas)
    std = np.nanstd(areas)
    mx = np.nanmax(areas)
    sl = slope_time(areas, years)
    zeros = int(np.sum(np.isclose(areas, 0.0)))

    recent = g[g["year"].isin(recent_years)]["area_km2"].sum()
    ratio_recent = (recent / total) if total > 0 else np.nan

    agg.append({
        "geocode_ibge": geo,
        "mun_norm": mun_norm,
        "uf_norm": uf_norm,
        "desmat_total_km2": total,
        "desmat_media_km2_ano": mean,
        "desmat_mediana_km2_ano": median,
        "desmat_std_km2": std,
        "desmat_max_km2": mx,
        "desmat_slope_km2_por_ano": sl,
        "anos_zero": zeros,
        "desmat_ult_3anos_km2": recent,
        "frac_ult_3anos": ratio_recent,
        "anos_disponiveis": int(len(g)),
    })

prodes_feat = pd.DataFrame(agg)
print("Features PRODES:", prodes_feat.shape)
prodes_feat.head()

"""### 4.2 Features de uso/cobertura (MapBiomas)

A partir da série (1985…2024), construímos indicadores por município:

- Natural, Anthropogenic e Farming no último ano
- Mudança entre primeiro e último ano
- Frações relativas no último ano (para comparar municípios de tamanhos diferentes)

"""

# ============================
# 4.2) Features MapBiomas por município
# ============================

mb_year_min, mb_year_max = min(year_cols), max(year_cols)

def build_mb_features(mb_l0: pd.DataFrame) -> pd.DataFrame:

    keep = ["Natural", "Antropic", "Farming (MapBiomas)"]

    mbk = mb_l0[mb_l0["class_level_0"].isin(keep)].copy()

    out_rows = []

    for (mun_norm, uf_norm), g in mbk.groupby(["mun_norm","uf_norm"]):

        row = {"mun_norm": mun_norm, "uf_norm": uf_norm}
        total_last = 0.0

        for cls in keep:

            gg = g[g["class_level_0"] == cls]

            a_first = float(gg[mb_year_min].sum()) if not gg.empty else np.nan
            a_last  = float(gg[mb_year_max].sum()) if not gg.empty else np.nan

            key = cls.lower().replace(" ", "_").replace("(", "").replace(")", "")

            row[f"mb_{key}__{mb_year_min}"] = a_first
            row[f"mb_{key}__{mb_year_max}"] = a_last
            row[f"mb_delta_{key}"] = (
                a_last - a_first
                if (not np.isnan(a_last) and not np.isnan(a_first))
                else np.nan
            )

            if not np.isnan(a_last):
                total_last += a_last

        row["mb_total_last"] = total_last if total_last > 0 else np.nan

        if total_last and not np.isnan(total_last) and total_last > 0:
            for cls in keep:
                key = cls.lower().replace(" ", "_").replace("(", "").replace(")", "")
                row[f"mb_frac_{key}__{mb_year_max}"] = (
                    row[f"mb_{key}__{mb_year_max}"] / total_last
                )

        out_rows.append(row)

    return pd.DataFrame(out_rows)

mb_feat = build_mb_features(mb_l0)

print("Features MapBiomas:", mb_feat.shape)
mb_feat.head()

"""### 4.3 Integração (PRODES + IBGE + MapBiomas)

- PRODES já possui `geocode_ibge`.
- IBGE fornece `geocode_ibge -> bioma predominante`.
- MapBiomas é associado por `(município, UF)` normalizados e então recebe `geocode_ibge`.

"""

# ============================
# 4.3) Integração
# ============================
geo_lookup = (
    prodes_long[["geocode_ibge","mun","uf"]]
    .dropna()
    .assign(
        mun_norm=lambda d: d["mun"].map(normalize_text),
        uf_norm=lambda d: d["uf"].map(normalize_text),
        geocode_ibge=lambda d: d["geocode_ibge"].astype(str).str.replace(r"\D","",regex=True),
    )
    .drop_duplicates(subset=["mun_norm","uf_norm","geocode_ibge"])
)

geo_lookup = geo_lookup.drop_duplicates(subset=["mun_norm","uf_norm"])

mb_feat2 = mb_feat.merge(geo_lookup[["mun_norm","uf_norm","geocode_ibge"]], on=["mun_norm","uf_norm"], how="left")
print("Proporção MapBiomas com geocode recuperado:", (mb_feat2["geocode_ibge"].notna().mean()).round(3))

# ==========================
# 4.3.1) MERGE CORRETO VIA IBGE
# ==========================

# Garantir normalização no IBGE
ibge_clean["mun_norm"] = ibge_clean["mun_ibge"].map(normalize_text)
ibge_clean["uf_norm"]  = ibge_clean["uf_ibge"].map(normalize_text)

# Garantir normalização no MapBiomas
mb_feat["mun_norm"] = mb_feat["mun_norm"].map(normalize_text)
mb_feat["uf_norm"]  = mb_feat["uf_norm"].map(normalize_text)

# Merge MapBiomas/IBGE para obter geocode
mb_feat2 = mb_feat.merge(
    ibge_clean[["geocode_ibge","mun_norm","uf_norm"]],
    on=["mun_norm","uf_norm"],
    how="left"
)

print("Proporção MapBiomas com geocode recuperado:",
      round(mb_feat2["geocode_ibge"].notna().mean(),4))

# Agora merge final por geocode
df = (
    prodes_feat
    .merge(ibge_clean[["geocode_ibge","bioma_ibge"]], on="geocode_ibge", how="left")
    .merge(mb_feat2.drop(columns=["mun_norm","uf_norm"]), on="geocode_ibge", how="left")
)

print("Dataset integrado:", df.shape)
df.head()

mb_frac_cols = [c for c in df.columns if c.startswith("mb_frac_")]

print("Cobertura MapBiomas nas features:")
for c in mb_frac_cols:
    print(c, round(df[c].notna().mean(), 4))

(df["mb_frac_natural__2024"]
 + df["mb_frac_antropic__2024"]
 + df["mb_frac_farming_mapbiomas__2024"]
).describe()

"""## 5) EDA

- Estatísticas descritivas
- Histogramas e boxplots (features PRODES)
- Correlação (Spearman) + heatmap

"""

numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
display(df[numeric_cols].describe().T)

print("Biomas (IBGE) presentes:", df["bioma_ibge"].dropna().unique())

to_plot = [
    "desmat_total_km2",
    "desmat_media_km2_ano",
    "desmat_slope_km2_por_ano",
    "desmat_ult_3anos_km2",
]
to_plot = [c for c in to_plot if c in df.columns]

for col in to_plot:
    plt.figure()
    df[col].replace([np.inf, -np.inf], np.nan).dropna().hist(bins=40)
    plt.title(f"Histograma - {col}")
    plt.xlabel(col)
    plt.ylabel("Frequência")
    plt.show()

for col in to_plot:
    plt.figure()
    plt.boxplot(df[col].replace([np.inf, -np.inf], np.nan).dropna(), vert=True)
    plt.title(f"Boxplot - {col}")
    plt.ylabel(col)
    plt.show()

corr_cols = [c for c in numeric_cols if df[c].notna().mean() > 0.7]
corr = df[corr_cols].corr(method="spearman")

plt.figure(figsize=(10,8))
plt.imshow(corr, aspect="auto")
plt.colorbar()
plt.xticks(range(len(corr_cols)), corr_cols, rotation=90)
plt.yticks(range(len(corr_cols)), corr_cols)
plt.title("Heatmap de Correlação (Spearman)")
plt.tight_layout()
plt.show()

"""## 6) Pré-processamento para clusterização

- seleção de features
- imputação (mediana)
- padronização (StandardScaler)

"""

# ==========================================
# 6) Pré-processamento FINAL para clusterização
# ==========================================

from sklearn.preprocessing import StandardScaler
import numpy as np

# Seleção de features
mb_last_year = mb_year_max
mb_last_year = mb_year_max

mb_frac_cols = [
    "mb_frac_natural__2024",
    "mb_frac_antropic__2024"
]

mb_frac_cols = [c for c in mb_frac_cols if c in df.columns]

base_cols = [
    "desmat_total_km2",
    "desmat_media_km2_ano",
    "desmat_std_km2",
    "desmat_slope_km2_por_ano",
    "desmat_ult_3anos_km2",
    "frac_ult_3anos",
    "anos_zero",
]

feat_cols = [c for c in base_cols if c in df.columns] + mb_frac_cols

X = df[feat_cols].copy()

# Remover infinitos
X = X.replace([np.inf, -np.inf], np.nan)

# Imputação
for col in X.columns:
    if X[col].isna().all():
        X[col] = 0
    else:
        X[col] = X[col].fillna(X[col].median())

# Garantia final
print("NaNs antes do scaler:", X.isna().sum().sum())

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Checagem FINAL obrigatória
print("NaNs após scaler:", np.isnan(X_scaled).sum())
print("Shape final:", X_scaled.shape)

"""## 7) PCA (visualização)

"""

# ============================
# 7) PCA
# ============================

from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np

# X deve ser o dataframe numérico das features (antes do scaler)
X_pca = df[feat_cols].replace([np.inf, -np.inf], np.nan).copy()

# imputação mediana
imputer = SimpleImputer(strategy="median")
X_imp = imputer.fit_transform(X_pca)

# padronização
scaler = StandardScaler()
X_scaled_pca = scaler.fit_transform(X_imp)

# checagem final
print("NaNs após imputação + scaler:", np.isnan(X_scaled_pca).sum())

pca = PCA(n_components=min(10, X_scaled_pca.shape[1]))
Z = pca.fit_transform(X_scaled_pca)

plt.figure()
plt.plot(np.cumsum(pca.explained_variance_ratio_), marker="o")
plt.title("PCA - Variância explicada acumulada")
plt.xlabel("Número de componentes")
plt.ylabel("Variância explicada acumulada")
plt.grid(True)
plt.show()

plt.figure()
plt.scatter(Z[:,0], Z[:,1], s=10)
plt.title("PCA 2D (sem rótulos)")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.show()

"""## 8) K-Means (Elbow + métricas)

"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

Ks = list(range(2, 11))
rows = []

for k in Ks:
    km = KMeans(
        n_clusters=k,
        random_state=42,
        n_init=20
    )

    labels = km.fit_predict(X_scaled)

    rows.append({
        "k": k,
        "sse": km.inertia_,
        "silhouette": silhouette_score(X_scaled, labels),
        "davies_bouldin": davies_bouldin_score(X_scaled, labels),
        "calinski_harabasz": calinski_harabasz_score(X_scaled, labels),
    })

metrics_k = pd.DataFrame(rows)

plt.figure()
plt.plot(metrics_k["k"], metrics_k["sse"], marker="o")
plt.title("Método do Cotovelo (SSE) - K-Means")
plt.xlabel("k")
plt.ylabel("SSE")
plt.grid(True)
plt.show()

display(metrics_k)

"""## 9) Estabilidade (ARI/NMI)

"""

best_k = int(metrics_k.sort_values("silhouette", ascending=False).iloc[0]["k"])
print("K escolhido (melhor silhouette):", best_k)

seeds = [0, 1, 2, 42, 100]
labels_by_seed = {}
for sd in seeds:
    km = KMeans(n_clusters=best_k, random_state=sd, n_init=10)
    labels_by_seed[sd] = km.fit_predict(X_scaled)

pairs = []
for i, s1 in enumerate(seeds):
    for s2 in seeds[i+1:]:
        pairs.append({
            "seed1": s1,
            "seed2": s2,
            "ARI": adjusted_rand_score(labels_by_seed[s1], labels_by_seed[s2]),
            "NMI": normalized_mutual_info_score(labels_by_seed[s1], labels_by_seed[s2]),
        })

stab = pd.DataFrame(pairs)
display(stab)
print("ARI médio:", stab["ARI"].mean().round(3), "| NMI médio:", stab["NMI"].mean().round(3))

"""## 10) Agrupamento Hierárquico (Ward)

Devido ao número elevado de amostras, o dendrograma completo torna-se visualmente poluído. Assim, foi utilizada a versão truncada do dendrograma, que evidencia apenas os níveis superiores da hierarquia, permitindo identificar claramente a formação de grandes grupos.

"""

Z_link = linkage(X_scaled, method="ward")

# Dendrograma truncado
plt.figure(figsize=(12,5))
dendrogram(Z_link, truncate_mode="lastp", p=30, leaf_rotation=90, leaf_font_size=10, show_contracted=True)
plt.title("Dendrograma (Ward) - TRUNCADO")
plt.xlabel("Cluster (agrupamento de folhas)")
plt.ylabel("Distância")
plt.tight_layout()
plt.show()

# Dendrograma com corte explícito (k = best_k)
labels_hier = fcluster(Z_link, t=best_k, criterion="maxclust")

plt.figure(figsize=(12,5))
dendrogram(Z_link, truncate_mode="lastp", p=30, leaf_rotation=90, leaf_font_size=10, show_contracted=True)
plt.title(f"Dendrograma (Ward) - TRUNCADO com corte (k={best_k})")
plt.xlabel("Cluster (agrupamento de folhas)")
plt.ylabel("Distância")
plt.tight_layout()
plt.show()

print("Silhouette (Ward):", silhouette_score(X_scaled, labels_hier).round(4))

"""## 11) DBSCAN (densidade)

"""

eps_values = [0.5, 0.8, 1.0, 1.2]
db_rows = []
for eps in eps_values:
    dbs = DBSCAN(eps=eps, min_samples=10)
    labels = dbs.fit_predict(X_scaled)
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    noise = float((labels == -1).mean())
    sil = np.nan
    if n_clusters >= 2 and noise < 1.0:
        mask = labels != -1
        if mask.sum() > 10 and len(set(labels[mask])) >= 2:
            sil = silhouette_score(X_scaled[mask], labels[mask])
    db_rows.append({"eps": eps, "n_clusters": n_clusters, "frac_noise": noise, "silhouette_no_noise": sil})

dbscan_metrics = pd.DataFrame(db_rows)
display(dbscan_metrics)

"""## 12) Interpretação (clusters + biomas)

"""

km_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
df["cluster_kmeans"] = km_final.fit_predict(X_scaled)

cluster_profile = df.groupby("cluster_kmeans")[feat_cols].mean()
display(cluster_profile)

if "bioma_ibge" in df.columns:
    cross = pd.crosstab(df["cluster_kmeans"], df["bioma_ibge"], normalize="index") * 100
    display(cross.round(2))

"""## ***Variância do PCA:**"""

print(pca.explained_variance_ratio_)

"""## ***KMeans vs Ward:**"""

print("Silhouette KMeans:", silhouette_score(X_scaled, df["cluster_kmeans"]))
print("Silhouette Ward:", silhouette_score(X_scaled, labels_hier))

"""## 13) Conclusões

A análise de agrupamentos identificou dois perfis principais de municípios brasileiros a partir de indicadores estruturais de desmatamento e uso do solo.

A redução de dimensionalidade via PCA mostrou que os primeiros componentes concentram grande parte da variância (~80% nos três primeiros), indicando que o fenômeno é explicado por poucos fatores dominantes, sobretudo intensidade acumulada, tendência temporal e dinâmica recente.

A escolha de **k = 2** foi sustentada por múltiplos critérios:

* Método do cotovelo (SSE)
* Maior Silhouette médio
* Alta estabilidade (ARI/NMI próximos de 1)
* Consistência com o agrupamento hierárquico (Ward)

Os métodos apresentaram desempenho elevado:

* Silhouette K-Means ~ 0,79
* Silhouette Ward ~ 0,80

Esses valores indicam boa separação entre os grupos e elevada coerência interna dos clusters.

---

## Perfil dos grupos identificados

### Cluster 0: Maior pressão de desmatamento

* Maior desmatamento total acumulado
* Tendência temporal positiva
* Maior intensidade recente (últimos anos)
* Proporção ainda relevante de cobertura natural

Esse perfil é compatível com municípios localizados em frentes ativas de expansão territorial, onde a presença de remanescentes naturais ainda permite continuidade da conversão do uso do solo.

---

### Cluster 1: Menor pressão ou estabilização

* Menor desmatamento acumulado
* Tendência estável ou negativa
* Maior proporção de áreas já consolidadas

Representa municípios com dinâmica recente reduzida, possivelmente associados a regiões com ocupação histórica consolidada ou maior controle territorial.

---

## Relação com uso do solo

As frações de cobertura Natural e Antrópica foram determinantes para diferenciar os grupos, evidenciando que a configuração territorial atual está fortemente associada ao histórico de desmatamento.

A variável *Farming* foi removida devido à colinearidade estrutural, já que as frações de uso do solo somam 1 por definição, o que poderia introduzir redundância na modelagem.

---

## Conclusão geral

O projeto demonstrou que técnicas de clusterização, quando combinadas com padronização adequada, análise exploratória e validação por múltiplas métricas, conseguem identificar padrões estruturais robustos de desmatamento e uso do solo em escala municipal.

Os agrupamentos obtidos foram:

* Estatisticamente consistentes;
* Estáveis sob diferentes inicializações;
* Interpretáveis do ponto de vista ambiental.

Os resultados revelam a existência de regimes territoriais distintos no Brasil, associados a diferentes estágios e dinâmicas de conversão do uso da terra.
"""